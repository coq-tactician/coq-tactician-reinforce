{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe62a95c-52f8-4a06-b8dc-e9e37e19f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of different nodes amd edges from given set\n",
    "# Put Decoder together \n",
    "# Create a full training pipeline \n",
    "\n",
    "# Remove recalculating graphs for the nodes that are already calculated as part of another graph.\n",
    "\n",
    "# Create Dataset\n",
    "# Create Encoder training pipeline [DONE] \n",
    "# Put cells together to create a Embedding Network [Done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e39b79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into PyTactician's visualizer.\n",
    "from pytact import data_reader, graph_visualize_browse\n",
    "import pathlib\n",
    "from typing import Optional, List, DefaultDict\n",
    "from pytact.data_reader import Node\n",
    "from pytact.graph_api_capnp_cython import EdgeClassification\n",
    "from pytact.graph_api_capnp_cython import Graph_Node_Label_Which\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dedbc3b-5aa2-4cab-91dc-4870383618d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_id(node):\n",
    "    prefix = f\"{node.graph}-{node.nodeid}\"\n",
    "    return prefix\n",
    "    \n",
    "def get_file_size(dataset_location,\n",
    "                 filename=\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"): \n",
    "    with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "        datapath = pathlib.Path(filename)\n",
    "        peano_dataset = reader[datapath] \n",
    "        pdl = peano_dataset.lowlevel\n",
    "        size = len(pdl.graph.nodes)\n",
    "        return size\n",
    "\n",
    "def get_dag(index, dataset_location, filename=\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\", max_iterations=100000000000000):\n",
    "    dag: DefaultDict[str, int] = defaultdict(int) \n",
    "    \n",
    "    with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "        datapath = pathlib.Path(filename)\n",
    "        peano_dataset = reader[datapath] \n",
    "    \n",
    "        # Define Graph to Traverse\n",
    "        current_node = peano_dataset.node_by_id(index)\n",
    "        # Initialize the required data structures\n",
    "        # Define initial variables\n",
    "        node_type_list: List[Graph_Node_Label_Which] = []\n",
    "        node_pytac_id_list: List[str] = []\n",
    "        edge_list: List[tuple[int,int]] = [] #paretnt_id, child_id,\n",
    "        edge_type_list: List[Optional[EdgeClassification]] = [] # edge_type\n",
    "        node_to_children_dict: DefaultDict[str, str] = defaultdict(list)\n",
    "        queue: List[tuple[Node, Optional[int], Optional[EdgeClassification]]] = [] #current_node, parent_id, edge_type\n",
    "        \n",
    "        # Initial Node Processing\n",
    "        current_node_pytac_id = get_node_id(current_node)\n",
    "        current_node_id = len(node_type_list)\n",
    "        dag[current_node_pytac_id] = current_node_id\n",
    "        node_type_list.append(current_node.label.which.name)\n",
    "        node_pytac_id_list.append(get_node_id(current_node))\n",
    "        \n",
    "        # Add children of the initial node to the queue\n",
    "        children = list(current_node.children)\n",
    "        if not current_node.label.which.name == 'REL': \n",
    "            for edge_label, child_node in children:\n",
    "                # Add child and edge details\n",
    "                queue.append((child_node, current_node_id, edge_label))\n",
    "                node_to_children_dict[get_node_id(current_node)].append(get_node_id(child_node)) \n",
    "        \n",
    "        # Other nodes processing \n",
    "        # Limit the traversal to a maximum depth or iterations\n",
    "        iteration = 1\n",
    "        while len(queue) and  iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            # Dequeue the next node\n",
    "            current_node, parent_id, edge_type = queue.pop(0)\n",
    "        \n",
    "            # If the node is in the graph then\n",
    "            # Add Edge, Dont expand Children, Dont add Node \n",
    "            current_node_pytac_id = get_node_id(current_node)\n",
    "            if current_node_pytac_id in dag: \n",
    "                # Add current existing node details\n",
    "                edge_list.append((parent_id, dag[current_node_pytac_id]))\n",
    "                edge_type_list.append(edge_type.name)\n",
    "                continue\n",
    "                \n",
    "            # Otherwise Add Edge, Node and Expand Children (besides children of REL node)    \n",
    "            else: \n",
    "                current_node_id = len(node_type_list)\n",
    "                dag[current_node_pytac_id] = current_node_id\n",
    "                # Add current node details\n",
    "                edge_list.append((parent_id, current_node_id))\n",
    "                edge_type_list.append(edge_type.name)\n",
    "                node_type_list.append(current_node.label.which.name)\n",
    "                node_pytac_id_list.append(get_node_id(current_node))\n",
    "                \n",
    "                # Process children of the current node\n",
    "                children = list(current_node.children)\n",
    "                if current_node.label.which.name == 'REL': \n",
    "                    continue\n",
    "                for edge_label, child_node in children:\n",
    "                    # Add child and edge details\n",
    "                    queue.append((child_node, current_node_id, edge_label))\n",
    "                    node_to_children_dict[get_node_id(current_node)].append(get_node_id(child_node)) \n",
    "            \n",
    "    return node_type_list, edge_list, edge_type_list, node_to_children_dict, node_pytac_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcdc35-cfa3-408a-9bc8-72675eb9aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 53, 53, 26, 36]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_location = '../../../../v15-stdlib-coq8.11/dataset'\n",
    "datapath = pathlib.Path(\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\")\n",
    "filename=\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"\n",
    "ss = get_dag(22, dataset_location, filename)\n",
    "[len(i) for i in ss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6fbd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        super(BasicRNN, self).__init__()\n",
    "\n",
    "        self.Wx = torch.randn(n_inputs, n_neurons) # n_inputs X n_neurons\n",
    "        self.Wh = torch.randn(n_neurons, n_neurons) # n_neurons X n_neurons\n",
    "\n",
    "        self.b = torch.zeros(1, n_neurons) # 1 X n_neurons\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        return torch.tanh(torch.mm(x, self.Wx) + torch.mm(hidden, self.Wh) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96315a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5614, -0.7634,  0.3944, -1.0000,  0.8981,  0.8435,  0.5908,  0.9964,\n",
      "         -0.9985, -1.0000, -1.0000, -0.9984,  1.0000, -0.9841, -0.6364,  0.9939,\n",
      "          0.9217, -0.9988,  0.9987,  0.9973]], grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BasicCSRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim, vocab_size, edges_size):\n",
    "        super(BasicCSRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.Wx = torch.randn(input_size, hidden_size) # n_inputs X n_neurons\n",
    "        self.We = torch.randn(edges_size, 1, hidden_size) # n_edges X 1 X n_neurons\n",
    "        self.Wh = torch.randn(hidden_size, hidden_size) # n_neurons X n_neurons\n",
    "        self.hidden_size =hidden_size\n",
    "        self.b = torch.zeros(1, hidden_size) # 1 X n_neurons\n",
    "\n",
    "    def forward(self, node):\n",
    "        return self.node_forward(node)\n",
    "\n",
    "    def node_forward(self, node):\n",
    "        x = self.embedding(torch.tensor(node.label.which.value))\n",
    "        x = x.unsqueeze(0)\n",
    "        if node.children and not node.label.which.name == 'REL':\n",
    "            hidden = torch.sum(torch.stack([self.node_forward(child)*self.We[edge_type.value] for edge_type, child in list(node.children)]), dim=0)\n",
    "            #hidden = torch.sum(torch.stack(self.node_forward(child) for child in node.children), dim=0)\n",
    "        else:\n",
    "            # Ensure that the zero tensor is of the correct shape [batch size, hidden size]\n",
    "            hidden = torch.zeros(x.size(0), self.hidden_size, dtype=torch.float, device=x.device)\n",
    "        return torch.tanh(torch.mm(x, self.Wx) + hidden + self.b)\n",
    "tmp = BasicCSRNN(10,20,10,100,100)\n",
    "index=3123\n",
    "with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "    datapath = pathlib.Path(filename)\n",
    "    peano_dataset = reader[datapath] \n",
    "\n",
    "    # Define Graph to Traverse\n",
    "    current_node = peano_dataset.node_by_id(index)\n",
    "    a = tmp.forward(current_node)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4b56d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0099, 0.0164, 0.0050, 0.0072, 0.0064, 0.0119, 0.0073, 0.0069, 0.0174,\n",
      "        0.0102, 0.0074, 0.0304, 0.0058, 0.0045, 0.0159, 0.0091, 0.0066, 0.0110,\n",
      "        0.0116, 0.0121, 0.0090, 0.0058, 0.0063, 0.0080, 0.0072, 0.0123, 0.0124,\n",
      "        0.0157, 0.0057, 0.0046, 0.0114, 0.0128, 0.0095, 0.0040, 0.0067, 0.0045,\n",
      "        0.0083, 0.0103, 0.0080, 0.0150, 0.0063, 0.0084, 0.0029, 0.0026, 0.0193,\n",
      "        0.0156, 0.0059, 0.0104, 0.0054, 0.0085, 0.0079, 0.0037, 0.0071, 0.0091,\n",
      "        0.0142, 0.0119, 0.0152, 0.0166, 0.0088, 0.0078, 0.0075, 0.0128, 0.0063,\n",
      "        0.0057, 0.0207, 0.0205, 0.0098, 0.0150, 0.0065, 0.0107, 0.0049, 0.0067,\n",
      "        0.0027, 0.0261, 0.0193, 0.0064, 0.0094, 0.0106, 0.0064, 0.0085, 0.0140,\n",
      "        0.0029, 0.0114, 0.0126, 0.0072, 0.0033, 0.0051, 0.0170, 0.0096, 0.0088,\n",
      "        0.0103, 0.0202, 0.0066, 0.0191, 0.0125, 0.0077, 0.0115, 0.0058, 0.0131,\n",
      "        0.0069], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23516/2296730677.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(logits)  # Applying softmax on the logits for probabilities\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class BasicCSRNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim, vocab_size, edges_size, output_size):\n",
    "        super(BasicCSRNNClassifier, self).__init__()\n",
    "        self.cs_rnn = BasicCSRNN(input_size, hidden_size, embedding_dim, vocab_size, edges_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)  # Linear classification layer\n",
    "\n",
    "    def forward(self, node):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        hn = self.cs_rnn(node)  # hn is the last hidden state\n",
    "        # output shape: (batch_size, sequence_length, hidden_size)\n",
    "        # hn shape: (1, batch_size, hidden_size)\n",
    "        hn = hn.squeeze(0)  # Remove the first dimension to match input of linear layer\n",
    "        logits = self.fc(hn)\n",
    "        probabilities = F.softmax(logits)  # Applying softmax on the logits for probabilities\n",
    "        return probabilities\n",
    "\n",
    "model = BasicCSRNNClassifier(10,20,10,100,100,100)\n",
    "index=3123\n",
    "with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "    datapath = pathlib.Path(filename)\n",
    "    peano_dataset = reader[datapath] \n",
    "\n",
    "    # Define Graph to Traverse\n",
    "    current_node = peano_dataset.node_by_id(index)\n",
    "    a = model.forward(current_node)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87b7e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TacticianCpuDatasetaset(Dataset):\n",
    "    def __init__(self, peano_dataset):\n",
    "        self.peano_dataset = peano_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10\n",
    "        # return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.peano_dataset.node_by_id(idx), torch.tensor(self.peano_dataset.node_by_id(idx).label.which.value)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa107b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Appropriate for classification tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "accumulation_steps = 10 \n",
    "\n",
    "with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "    datapath = pathlib.Path(filename)\n",
    "    peano_dataset = reader[datapath] \n",
    "    \n",
    "    custom_dataset = TacticianCpuDatasetaset(peano_dataset)\n",
    "\n",
    "    for epoch in range(5):  # Loop over the dataset multiple times\n",
    "        for i in range(3100):#random.sample(range(3100), 400): \n",
    "            graph, label = custom_dataset.__getitem__(i)\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(graph)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss = loss / accumulation_steps  # Normalize our loss (if averaged)\n",
    "\n",
    "            # Backward pass (accumulates gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:  # Perform a step every 'accumulation_steps'\n",
    "                optimizer.step()  # Update parameters\n",
    "                optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "            #print(f'Batch {i+1}')\n",
    "        print(f'Epoch {epoch+1}/{5} Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ae0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "predictions = []\n",
    "actuals = []\n",
    "with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "    datapath = pathlib.Path(filename)\n",
    "    peano_dataset = reader[datapath] \n",
    "    \n",
    "    custom_dataset = TacticianCpuDatasetaset(peano_dataset)\n",
    "    for i in range(3100): #random.sample(range(3100), 300):\n",
    "        graph, label = custom_dataset.__getitem__(i)\n",
    "        predictions.append(torch.argmax(model.forward(graph), dim=0))\n",
    "        actuals.append(label)\n",
    "        \n",
    "        #print(torch.argmax((model.forward(graph))), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d20ae7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       213\n",
      "           2       1.00      1.00      1.00        62\n",
      "           3       0.99      1.00      1.00       187\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         1\n",
      "           7       0.00      0.00      0.00         1\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       1.00      1.00      1.00       485\n",
      "          10       0.99      1.00      1.00       195\n",
      "          11       1.00      1.00      1.00       183\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       1.00      1.00      1.00       358\n",
      "          14       1.00      1.00      1.00       455\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.99      1.00      0.99       782\n",
      "          17       0.99      1.00      0.99        79\n",
      "          18       0.99      1.00      0.99        86\n",
      "\n",
      "    accuracy                           1.00      3100\n",
      "   macro avg       0.61      0.61      0.61      3100\n",
      "weighted avg       0.99      1.00      0.99      3100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tactician/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/tactician/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/tactician/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "report = classification_report(actuals, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aba4e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[node-170-3123]\n"
     ]
    }
   ],
   "source": [
    "with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "    datapath = pathlib.Path(filename)\n",
    "    peano_dataset = reader[datapath] \n",
    "    current_node = peano_dataset.node_by_id(index)\n",
    "    print([current_node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de74c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pygeometric dataset\n",
    "# Train the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "18d9b44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'170-187': 0, '170-1': 1, '170-0': 2, '170-188': 0})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_location = '../../../../v15-stdlib-coq8.11/dataset'\n",
    "datapath = pathlib.Path(\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\")\n",
    "filename=\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"\n",
    "a = Tree(0, dataset_location, filename)\n",
    "get_dag(0, dataset_location, filename)\n",
    "get_file_size(dataset_location, filename)\n",
    "def get_pytac_node_id(index, dataset_location, filename=\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"):    \n",
    "    with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "        datapath = pathlib.Path(filename)\n",
    "        peano_dataset = reader[datapath] \n",
    "        current_node = peano_dataset.node_by_id(index)\n",
    "        node_pytac_id = get_node_id(current_node)\n",
    "    return node_pytac_id\n",
    "\n",
    "\n",
    "def get_graph_nodes_depth(index, dataset_location, filename=\"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"):    \n",
    "    depth = defaultdict(int)  # None indicates not yet processed\n",
    "    visited = defaultdict(bool)\n",
    "    stack = []  \n",
    "    with data_reader.data_reader(pathlib.Path(dataset_location)) as reader:\n",
    "        datapath = pathlib.Path(filename)\n",
    "        peano_dataset = reader[datapath] \n",
    "        current_node = peano_dataset.node_by_id(index)\n",
    "        # Process initial node \n",
    "\n",
    "        stack.append((current_node, 0))\n",
    "        depth\n",
    "        while stack: \n",
    "            current_node, state = stack.pop()\n",
    "            node_pytac_id = get_node_id(current_node)\n",
    "            if state == 0:  # First time visiting the node\n",
    "                    stack.append((current_node, 1))  # Push back with state 1\n",
    "                    # Add all children to the stack\n",
    "                    for _, child in list(current_node.children):\n",
    "\n",
    "                        if get_node_id(child) not in visited:  # Process only unprocessed children\n",
    "                            stack.append((child, 0))\n",
    "                            visited[get_node_id(child)]=True\n",
    "            else: # all children processed\n",
    "                if len(list(current_node.children))==0 or current_node.label.which.name=='REL': \n",
    "                    depth[node_pytac_id] = 0\n",
    "                else:\n",
    "                    depth[node_pytac_id] = max([depth[get_node_id(child)] for _,child in list(current_node.children)])+1\n",
    "    return depth\n",
    "get_current_node_id(0, dataset_location)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d852f99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./graph_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     58\u001b[0m dataset \u001b[38;5;241m=\u001b[39m PytacPygeomDataset(root_dir, dataset_location, [filename])\n\u001b[0;32m---> 59\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Create and save the dataset\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[99], line 25\u001b[0m, in \u001b[0;36mPytacPygeomDataset._process\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(index)\n\u001b[1;32m     23\u001b[0m dag_info \u001b[38;5;241m=\u001b[39m get_dag(index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_location, filename\u001b[38;5;241m=\u001b[39mfilename, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000000000000\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m execution_order_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_nodes_depth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m dag_info[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# node_attributes\u001b[39;00m\n\u001b[1;32m     27\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m dag_info[\u001b[38;5;241m1\u001b[39m] \n",
      "Cell \u001b[0;32mIn[89], line 20\u001b[0m, in \u001b[0;36mget_graph_nodes_depth\u001b[0;34m(index, dataset_location, filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m visited \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     19\u001b[0m stack \u001b[38;5;241m=\u001b[39m []  \n\u001b[0;32m---> 20\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeano_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatapath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/site-packages/pytact/data_reader.pyx:1270\u001b[0m, in \u001b[0;36mdata_reader\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/site-packages/pytact/data_reader.pyx:277\u001b[0m, in \u001b[0;36mlowlevel_data_reader\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/site-packages/pytact/data_reader.pyx:288\u001b[0m, in \u001b[0;36mpytact.data_reader.lowlevel_data_reader\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/site-packages/pytact/data_reader.pyx:208\u001b[0m, in \u001b[0;36mpytact.data_reader.LowlevelDataReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/pathlib.py:871\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m--> 871\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    874\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/pathlib.py:509\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args):\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 509\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/pathlib.py:502\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    499\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument should be a str object or an os.PathLike \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject returning str, not \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    501\u001b[0m                 \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(a))\n\u001b[0;32m--> 502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flavour\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/tactician/lib/python3.11/pathlib.py:75\u001b[0m, in \u001b[0;36m_Flavour.parse_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rel \u001b[38;5;129;01mand\u001b[39;00m rel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m         parsed\u001b[38;5;241m.\u001b[39mappend(sys\u001b[38;5;241m.\u001b[39mintern(rel))\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drv \u001b[38;5;129;01mor\u001b[39;00m root:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m drv:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m# If no drive is present, try to find one in the previous\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;66;03m# parts. This makes the result of parsing e.g.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m# (\"C:\", \"/\", \"a\") reasonably intuitive.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m it:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Dataset, Data\n",
    "\n",
    "# Define a custom dataset class with an option to save and load\n",
    "class PytacPygeomDataset(Dataset):\n",
    "    def __init__(self, root, dataset_location, files_list, transform=None, pre_transform=None):\n",
    "        super().__init__(root, dataset_location, files_list, transform, pre_transform)\n",
    "        self.root = root\n",
    "        self.graphs = []\n",
    "        self.dataset_location = dataset_location\n",
    "        self.files_list = files_list\n",
    "    \n",
    "    def _process(self):\n",
    "        # Example: Generate some graphs\n",
    "        for filename in self.files_list:    \n",
    "            size = get_file_size(self.dataset_location, filename=filename)\n",
    " \n",
    "\n",
    "            for index in range(size):\n",
    "                if not index%100: \n",
    "                    print(index)\n",
    "                dag_info = get_dag(index, self.dataset_location, filename=filename, max_iterations=100000000000000)\n",
    "                \n",
    "                execution_order_dict = get_graph_nodes_depth(index, self.dataset_location, filename=filename)\n",
    "                x = dag_info[0] # node_attributes\n",
    "                edge_index = dag_info[1] \n",
    "                edge_attr = dag_info[2]\n",
    "                execution_order = [execution_order_dict[i] for i in dag_info[4]] # execution order for the TreeLstm from low to high - first Leaf nodes\n",
    "\n",
    "                self.graphs.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, execution_order=execution_order))\n",
    "\n",
    "            # Save the processed data\n",
    "        self.save_to_disk()\n",
    "    \n",
    "    def save_to_disk(self):\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "        for idx, graph in enumerate(self.graphs):\n",
    "            path = os.path.join(self.processed_dir, f'graph_{idx}.pt')\n",
    "            torch.save(graph, path)\n",
    "    \n",
    "    def load_from_disk(self):\n",
    "        self.graphs = []\n",
    "        for filename in os.listdir(self.processed_dir):\n",
    "            if filename.endswith('.pt'):\n",
    "                path = os.path.join(self.processed_dir, filename)\n",
    "                graph = torch.load(path)\n",
    "                self.graphs.append(graph)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "# Define paths and process dataset\n",
    "root_dir = './graph_dataset'\n",
    "dataset = PytacPygeomDataset(root_dir, dataset_location, [filename])\n",
    "dataset._process()  # Create and save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7f017ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['DEFINITION',\n",
       "  'LAMBDA',\n",
       "  'PROD',\n",
       "  'SORT_TYPE',\n",
       "  'LAMBDA',\n",
       "  'PROD',\n",
       "  'REL',\n",
       "  'LAMBDA',\n",
       "  'REL',\n",
       "  'PROD',\n",
       "  'DEFINITION',\n",
       "  'CASE',\n",
       "  'SORT_PROP',\n",
       "  'DEFINITION',\n",
       "  'LAMBDA',\n",
       "  'REL',\n",
       "  'CASE_BRANCH',\n",
       "  'REL',\n",
       "  'REL'],\n",
       " [(0, 1),\n",
       "  (0, 2),\n",
       "  (1, 3),\n",
       "  (1, 4),\n",
       "  (2, 3),\n",
       "  (2, 5),\n",
       "  (4, 6),\n",
       "  (4, 7),\n",
       "  (5, 8),\n",
       "  (5, 9),\n",
       "  (7, 10),\n",
       "  (7, 11),\n",
       "  (9, 10),\n",
       "  (9, 8),\n",
       "  (10, 12),\n",
       "  (10, 13),\n",
       "  (11, 10),\n",
       "  (11, 14),\n",
       "  (11, 15),\n",
       "  (11, 16),\n",
       "  (13, 17),\n",
       "  (14, 10),\n",
       "  (14, 6),\n",
       "  (16, 13),\n",
       "  (16, 18)],\n",
       " ['CONST_DEF',\n",
       "  'CONST_TYPE',\n",
       "  'LAMBDA_TYPE',\n",
       "  'LAMBDA_TERM',\n",
       "  'PROD_TYPE',\n",
       "  'PROD_TERM',\n",
       "  'LAMBDA_TYPE',\n",
       "  'LAMBDA_TERM',\n",
       "  'PROD_TYPE',\n",
       "  'PROD_TERM',\n",
       "  'LAMBDA_TYPE',\n",
       "  'LAMBDA_TERM',\n",
       "  'PROD_TYPE',\n",
       "  'PROD_TERM',\n",
       "  'IND_TYPE',\n",
       "  'IND_CONSTRUCT',\n",
       "  'CASE_IND',\n",
       "  'CASE_RETURN',\n",
       "  'CASE_TERM',\n",
       "  'CASE_BRANCH_POINTER',\n",
       "  'CONSTRUCT_TERM',\n",
       "  'LAMBDA_TYPE',\n",
       "  'LAMBDA_TERM',\n",
       "  'C_B_CONSTRUCT',\n",
       "  'C_B_TERM'],\n",
       " [8, 7, 5, 0, 6, 4, 0, 5, 0, 3, 2, 4, 0, 1, 3, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dataset.get(2)\n",
    "a.x, a.edge_index, a.edge_attr,a.execution_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bb2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 7, 5, 0, 6, 4, 0, 5, 0, 3, 2, 4, 0, 1, 3, 0, 2, 0, 0],\n",
       " ['DEFINITION',\n",
       "  'LAMBDA',\n",
       "  'PROD',\n",
       "  'SORT_TYPE',\n",
       "  'LAMBDA',\n",
       "  'PROD',\n",
       "  'REL',\n",
       "  'LAMBDA',\n",
       "  'REL',\n",
       "  'PROD',\n",
       "  'DEFINITION',\n",
       "  'CASE',\n",
       "  'SORT_PROP',\n",
       "  'DEFINITION',\n",
       "  'LAMBDA',\n",
       "  'REL',\n",
       "  'CASE_BRANCH',\n",
       "  'REL',\n",
       "  'REL'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#node_type_list, edge_list, edge_type_list, node_to_children_dict, node_pytac_id_list\n",
    "id = 2\n",
    "ss = get_dag(id, dataset_location, filename)\n",
    "st = get_current_node_id(id, dataset_location)   \n",
    "\n",
    "x = ss[0] # node_attributes\n",
    "edge_list = ss[1] \n",
    "edge_attr = ss[2]\n",
    "distnace_from_leaf = [st[i] for i in ss[4]] # execution order for the TreeLstm from low to high - first Leaf nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactician",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
