{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3e05b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kj/filesystem-disk-unix.c++:1703: warning: PWD environment variable doesn't match current directory; pwd = /root\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into PyTactician's visualizer.\n",
    "from pytact import data_reader, graph_visualize_browse\n",
    "import pathlib\n",
    "from typing import Optional, List, DefaultDict\n",
    "from pytact.data_reader import Node\n",
    "from pytact.graph_api_capnp_cython import EdgeClassification\n",
    "from pytact.graph_api_capnp_cython import Graph_Node_Label_Which\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "#from sklearn.metrics import classification_report\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3150d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TreeLSTMCell(nn.Module):\n",
    "    def __init__(self, hidden_size: int, edges_number: int):\n",
    "        super(TreeLSTMCell, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.edges_number = edges_number\n",
    "\n",
    "        # Parameters for the LSTM cell\n",
    "        # Note: Each edge type has its own set of LSTM parameters\n",
    "        self.W_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.W_f = nn.Parameter(torch.randn(edges_number, hidden_size, hidden_size))\n",
    "        self.W_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.W_c = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        \n",
    "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        self.U_c = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
    "        \n",
    "        self.b_i = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.randn(hidden_size))\n",
    "\n",
    "    def forward(self, h_, c_, e, x: torch.Tensor):\n",
    "        # h_ and c_ are lists of hidden states and cell states of child nodes\n",
    "        # e is a list of edge types\n",
    "\n",
    "        H, C = torch.stack(h_), torch.stack(c_)\n",
    "        # Gates calculations\n",
    "        i_t = torch.sigmoid(torch.mean(torch.stack([\n",
    "            torch.mm(h, self.W_i) for h, _ in zip(H, e)\n",
    "        ]), dim=0) + torch.mm(x, self.U_i) + self.b_i)\n",
    "\n",
    "        f_t = torch.sigmoid(torch.mean(torch.stack([\n",
    "            torch.mm(h, self.W_f[edge_idx]) for h, edge_idx in zip(H, e)\n",
    "        ]), dim=0) + torch.mm(x, self.U_f) + self.b_f)\n",
    "\n",
    "        o_t = torch.sigmoid(torch.mean(torch.stack([\n",
    "            torch.mm(h, self.W_o) for h, _ in zip(H, e)\n",
    "        ]), dim=0) + torch.mm(x, self.U_o) + self.b_o)\n",
    "\n",
    "        c_hat_t = torch.tanh(torch.mean(torch.stack([\n",
    "            torch.mm(h, self.W_c) for h, edge_idx in zip(H, e)\n",
    "        ]), dim=0) + torch.mm(x, self.U_c) + self.b_c)\n",
    "\n",
    "        # Calculate cell state\n",
    "        C_t = torch.sum(f_t * C, dim=0) + i_t * c_hat_t\n",
    "\n",
    "        # Calculate hidden state\n",
    "        h_t = o_t * torch.tanh(C_t)\n",
    "        return h_t.squeeze(1), C_t.squeeze(1)\n",
    "\n",
    "\n",
    "class DecodeEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size: int, edges_number: int):\n",
    "        super(DecodeEmbedding, self).__init__()\n",
    "        # Initialize the Ep matrix as a trainable parameter\n",
    "        self.Ep = nn.Parameter(torch.randn(edges_number, hidden_size, hidden_size))\n",
    "        # Initialize cp as a trainable parameter \n",
    "        self.cp = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "    def forward(self, h: torch.Tensor, edge_idx: int) -> torch.Tensor:\n",
    "        # Ensure cp is broadcasted correctly over the batches\n",
    "        transformed_h = torch.mm(h, self.Ep[edge_idx])\n",
    "        # Add cp (broadcasted) to the result of the matrix multiplication\n",
    "        # Note: If cp is intended to be a fixed bias, it should be initialized outside the parameter list\n",
    "        return torch.sigmoid(transformed_h + self.cp)  # squeeze cp to match dimensions\n",
    "    \n",
    "\n",
    "class LSTMEncoderDecoderClasifier(nn.Module): \n",
    "    def __init__(self, hidden_size: int, edges_number: int, nodes_number: int): \n",
    "        super(LSTMEncoderDecoderClasifier, self).__init__()\n",
    "        self.emb = nn.Embedding(nodes_number, hidden_size)\n",
    "        self.dec = DecodeEmbedding(hidden_size, edges_number)\n",
    "        self.enc = TreeLSTMCell(hidden_size, edges_number)\n",
    "        self.R = nn.Linear(hidden_size, nodes_number, bias=True) #output\n",
    "      \n",
    "    def encode_dag(self, dag):\n",
    "        def encode_node(node): \n",
    "            if node.children and node.label.which.name != 'REL': \n",
    "                h_list = [] \n",
    "                c_list = []\n",
    "                e_list = []\n",
    "                for edge_label, child in node.children:\n",
    "                    h, c = encode_node(child)\n",
    "                    h_list.append(h)\n",
    "                    c_list.append(c)\n",
    "                    e_list.append(edge_label.value)\n",
    "                return self.enc(h_list, c_list, e_list, self.emb(torch.tensor(node.label.which.value)).unsqueeze(0))\n",
    "            else: \n",
    "                h = self.emb(torch.tensor(node.label.which.value)).unsqueeze(0)\n",
    "                c = self.emb(torch.tensor(node.label.which.value)).unsqueeze(0)\n",
    "                return h, c\n",
    "        h, _ = encode_node(dag)\n",
    "        return h\n",
    "    \n",
    "    def decode_dag(self, dag, h, max_depth):\n",
    "        decoded_graph = []\n",
    "        def decode_node(node, h, depth, max_depth):\n",
    "            logits = self.R(h)\n",
    "            decoded_graph.append((torch.softmax(logits, dim=-1), depth))\n",
    "            if depth < max_depth and node.children and not node.label.which.name == 'REL':\n",
    "                for edge_label, child in node.children: \n",
    "                    h = self.dec(h, edge_label.value)\n",
    "                    decode_node(child, h, depth+1, max_depth)\n",
    "\n",
    "        decode_node(dag, h, 1, max_depth)\n",
    "        return decoded_graph\n",
    "    \n",
    "    def forward(self, dag, max_depth):\n",
    "        enc = self.encode_dag(dag)\n",
    "        return self.decode_dag(dag, enc, max_depth)\n",
    "\n",
    "    \n",
    "class LabelGetter: \n",
    "    def __init__(self): \n",
    "        self.labels = []\n",
    "    def get_labels(self, graph, max_depth):\n",
    "        self.labels = []\n",
    "        self.get_labels_helper(graph, 1, max_depth)\n",
    "        return self.labels\n",
    "    def get_labels_helper(self, graph, depth, max_depth):\n",
    "        self.labels.append(graph.label.which.value)\n",
    "        if graph.children and not graph.label.which.name == 'REL' and depth < max_depth: \n",
    "            for _, child in list(graph.children):\n",
    "                self.get_labels_helper(child, depth+1, max_depth)\n",
    "                \n",
    "def get_file_size(reader, dataset_pointer): \n",
    "        pdl = dataset_pointer.lowlevel\n",
    "        size = len(pdl.graph.nodes)\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e5c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configurations\n",
    "DATASET_PATH = '../../../v15-stdlib-coq8.11/dataset'\n",
    "FILE_PATH = \"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"\n",
    "DATASET_PATH = pathlib.Path(DATASET_PATH)\n",
    "FILE_PATH = pathlib.Path(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f62ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Model Parameters \n",
    "NODES_NUMBER = 30\n",
    "EMBEDDING_SIZE = 8\n",
    "HIDDEN_SIZE = 16\n",
    "EDGES_NUMBER = 50\n",
    "\n",
    "# Model Introduction\n",
    "model = LSTMEncoderDecoderClasifier(HIDDEN_SIZE, EDGES_NUMBER, NODES_NUMBER)\n",
    "lg = LabelGetter() #graph node_labels extractor\n",
    "\n",
    "# Model Training Details\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 20\n",
    "MAX_DECODING_DEPTH = 3\n",
    "EPOCHS = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "983ac4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max decoding depth: 1, Epoch 1/3, Training Loss: 3.128580997090639, TrainingAccuracy: 52.36%, Test Accuracy: 63.45%\n",
      "Max decoding depth: 1, Epoch 2/3, Training Loss: 2.794567477263431, TrainingAccuracy: 70.91%, Test Accuracy: 77.78%\n",
      "Max decoding depth: 1, Epoch 3/3, Training Loss: 2.6824124881675706, TrainingAccuracy: 85.04%, Test Accuracy: 87.44%\n",
      "Max decoding depth: 2, Epoch 1/3, Training Loss: 1.2233384361171864, TrainingAccuracy: 37.36%, Test Accuracy: 44.68%\n",
      "Max decoding depth: 2, Epoch 2/3, Training Loss: 1.196088734693178, TrainingAccuracy: 48.20%, Test Accuracy: 52.87%\n",
      "Max decoding depth: 2, Epoch 3/3, Training Loss: 1.1650459589897346, TrainingAccuracy: 59.30%, Test Accuracy: 61.35%\n",
      "Max decoding depth: 3, Epoch 1/3, Training Loss: 0.8138397572566332, TrainingAccuracy: 53.95%, Test Accuracy: 60.39%\n",
      "Max decoding depth: 3, Epoch 2/3, Training Loss: 0.8096095865061339, TrainingAccuracy: 62.22%, Test Accuracy: 63.95%\n",
      "Max decoding depth: 3, Epoch 3/3, Training Loss: 0.8060436052268769, TrainingAccuracy: 63.68%, Test Accuracy: 63.82%\n"
     ]
    }
   ],
   "source": [
    "with data_reader.data_reader(DATASET_PATH) as reader:\n",
    "    dataset_pointer = reader[FILE_PATH]      \n",
    "    grpahs_number = get_file_size(reader, dataset_pointer)\n",
    "    shuffled_indexes = list(range(grpahs_number)) # change indexes to random_shuffle\n",
    "    random.shuffle(shuffled_indexes)\n",
    "    train_indexes = shuffled_indexes[:grpahs_number*7//10]\n",
    "    test_indexes = shuffled_indexes[grpahs_number*7//10:]\n",
    "    for max_depth in range(1, MAX_DECODING_DEPTH+1):\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Training Loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            for i in train_indexes:\n",
    "                graph = dataset_pointer.node_by_id(i)\n",
    "                labels = lg.get_labels(graph, max_depth)\n",
    "                optimizer.zero_grad()\n",
    "                output_whole = model(graph, max_depth)    \n",
    "                output_whole = [j for j,_ in output_whole]\n",
    "                loss = criterion(torch.stack(output_whole).squeeze(1), torch.tensor(labels))/len(labels)\n",
    "                loss.backward()\n",
    "                if (i + 1) % BATCH_SIZE == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(torch.stack(output_whole).squeeze(1), dim=1)\n",
    "                correct += (predictions == torch.tensor(labels)).sum().item()\n",
    "                total += len(labels)\n",
    "            trainig_accuracy = correct / total if total > 0 else 0\n",
    "            \n",
    "            # Testing Loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in test_indexes:\n",
    "                graph = dataset_pointer.node_by_id(i)\n",
    "                labels = lg.get_labels(graph, max_depth)\n",
    "                with torch.no_grad():\n",
    "                    output_whole = model(graph, max_depth=max_depth)\n",
    "\n",
    "                    output_whole = [j for j,_ in output_whole]\n",
    "                    predictions = torch.argmax(torch.stack(output_whole).squeeze(1), dim=1)\n",
    "                    correct += (predictions == torch.tensor(labels)).sum().item()\n",
    "                    total += len(labels)\n",
    "            \n",
    "            accuracy = correct / total if total > 0 else 0\n",
    "            print(f'Max decoding depth: {max_depth}, Epoch {epoch+1}/{EPOCHS}, Training Loss: {total_loss / len(train_indexes)}, TrainingAccuracy: {trainig_accuracy* 100:.2f}%, Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactician-ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
