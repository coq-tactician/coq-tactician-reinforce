{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3e05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into PyTactician's visualizer.\n",
    "from pytact import data_reader, graph_visualize_browse\n",
    "import pathlib\n",
    "from typing import Optional, List, DefaultDict\n",
    "from pytact.data_reader import Node\n",
    "from pytact.graph_api_capnp_cython import EdgeClassification\n",
    "from pytact.graph_api_capnp_cython import Graph_Node_Label_Which\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3150d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TreeRNNCell(nn.Module):\n",
    "    def __init__(self, hidden_size: int, edges_number: int):\n",
    "        super(TreeRNNCell, self).__init__()\n",
    "        # Initialize the Up matrix as a trainable parameter\n",
    "        self.Up = nn.Parameter(torch.randn(edges_number, hidden_size, hidden_size))\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, h_, e, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Calculate the mean of transformed child hidden states\n",
    "        child_agg = torch.mean(torch.stack([\n",
    "            torch.mm(h, self.Up[edge_idx]) for h, edge_idx in zip(h_,e)\n",
    "        ]), dim=0)\n",
    "     \n",
    "        # Apply non-linear transformation and sum with input tensor x\n",
    "        return torch.tanh(child_agg + x)\n",
    "\n",
    "\n",
    "class DecodeEmbedding(nn.Module):\n",
    "    def __init__(self, hidden_size: int, edges_number: int):\n",
    "        super(DecodeEmbedding, self).__init__()\n",
    "        # Initialize the Ep matrix as a trainable parameter\n",
    "        self.Ep = nn.Parameter(torch.randn(edges_number, hidden_size, hidden_size))\n",
    "        # Initialize cp as a trainable parameter \n",
    "        self.cp = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "    def forward(self, h: torch.Tensor, edge_idx: int) -> torch.Tensor:\n",
    "        # Ensure cp is broadcasted correctly over the batches\n",
    "        transformed_h = torch.mm(h, self.Ep[edge_idx])\n",
    "        # Add cp (broadcasted) to the result of the matrix multiplication\n",
    "        # Note: If cp is intended to be a fixed bias, it should be initialized outside the parameter list\n",
    "        return torch.sigmoid(transformed_h + self.cp)  # squeeze cp to match dimensions\n",
    "\n",
    "\n",
    "\n",
    "class RNNEncoderDecoderClasifier(nn.Module): \n",
    "    def __init__(self, hidden_size: int, edges_number: int, nodes_number: int): \n",
    "        super(RNNEncoderDecoderClasifier, self).__init__()\n",
    "        self.emb = nn.Embedding(nodes_number, hidden_size)\n",
    "        self.dec = DecodeEmbedding(hidden_size, edges_number)\n",
    "        self.enc = TreeRNNCell(hidden_size, edges_number)\n",
    "        self.R = nn.Linear(hidden_size, nodes_number, bias=True) #output\n",
    "      \n",
    "    def encode_dag(self, dag):\n",
    "        def encode_node(node): \n",
    "            if node.children and node.label.which.name != 'REL': \n",
    "                h = [] \n",
    "                e = [] \n",
    "                for edge_label, child in node.children: \n",
    "                    h.append(encode_node(child))\n",
    "                    e.append(edge_label.value)\n",
    "                return self.enc(h, e, self.emb(torch.tensor(node.label.which.value)))\n",
    "            else: \n",
    "                return self.emb(torch.tensor(node.label.which.value)).unsqueeze(0)\n",
    "        encoding = encode_node(dag)\n",
    "        return encoding\n",
    "    \n",
    "    def decode_dag(self, dag, h, max_depth):\n",
    "        decoded_graph = []\n",
    "        def decode_node(node, h, depth, max_depth):\n",
    "            logits = self.R(h)\n",
    "            decoded_graph.append((torch.softmax(logits, dim=-1), depth))\n",
    "            if depth < max_depth and node.children and not node.label.which.name == 'REL':\n",
    "                for edge_label, child in node.children: \n",
    "                    h = self.dec(h, edge_label.value)\n",
    "                    decode_node(child, h, depth+1, max_depth)\n",
    "\n",
    "        decode_node(dag, h, 1, max_depth)\n",
    "        return decoded_graph\n",
    "    \n",
    "    def forward(self, dag, max_depth):\n",
    "        enc = self.encode_dag(dag)\n",
    "        return self.decode_dag(dag, enc, max_depth)\n",
    "    \n",
    "class LabelGetter: \n",
    "    def __init__(self): \n",
    "        self.labels = []\n",
    "    def get_labels(self, graph, max_depth):\n",
    "        self.labels = []\n",
    "        self.get_labels_helper(graph, 1, max_depth)\n",
    "        return self.labels\n",
    "    def get_labels_helper(self, graph, depth, max_depth):\n",
    "        self.labels.append(graph.label.which.value)\n",
    "        if graph.children and not graph.label.which.name == 'REL' and depth < max_depth: \n",
    "            for _, child in list(graph.children):\n",
    "                self.get_labels_helper(child, depth+1, max_depth)\n",
    "                \n",
    "def get_file_size(reader, dataset_pointer): \n",
    "        pdl = dataset_pointer.lowlevel\n",
    "        size = len(pdl.graph.nodes)\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e5c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configurations\n",
    "DATASET_PATH = '../../../../v15-stdlib-coq8.11/dataset'\n",
    "FILE_PATH = \"coq-tactician-stdlib.8.11.dev/theories/Init/Logic.bin\"\n",
    "DATASET_PATH = pathlib.Path(DATASET_PATH)\n",
    "FILE_PATH = pathlib.Path(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f62ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Model Parameters \n",
    "NODES_NUMBER = 30\n",
    "EMBEDDING_SIZE = 8\n",
    "HIDDEN_SIZE = 16\n",
    "EDGES_NUMBER = 50\n",
    "\n",
    "# Model Introduction\n",
    "model = RNNEncoderDecoderClasifier(HIDDEN_SIZE, EDGES_NUMBER, NODES_NUMBER)\n",
    "lg = LabelGetter() #graph node_labels extractor\n",
    "\n",
    "# Model Training Details\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 20\n",
    "MAX_DECODING_DEPTH = 3\n",
    "EPOCHS = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983ac4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max decoding depth: 1, Epoch 1/3, Training Loss: 3.4073306798934935, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 1, Epoch 2/3, Training Loss: 3.4073306798934935, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 1, Epoch 3/3, Training Loss: 3.4073306798934935, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 2, Epoch 1/3, Training Loss: 1.2484236359596252, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 2, Epoch 2/3, Training Loss: 1.2484236359596252, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 2, Epoch 3/3, Training Loss: 1.2484236359596252, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 3, Epoch 1/3, Training Loss: 0.7662498563528061, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 3, Epoch 2/3, Training Loss: 0.7662498563528061, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n",
      "Max decoding depth: 3, Epoch 3/3, Training Loss: 0.7662498563528061, TrainingAccuracy: 0.00%, Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with data_reader.data_reader(DATASET_PATH) as reader:\n",
    "    dataset_pointer = reader[FILE_PATH]      \n",
    "    grpahs_number = get_file_size(reader, dataset_pointer)\n",
    "    shuffled_indexes = list(range(grpahs_number)) # change indexes to random_shuffle\n",
    "    random.shuffle(shuffled_indexes)\n",
    "    train_indexes = shuffled_indexes[:grpahs_number*7//10]\n",
    "    test_indexes = shuffled_indexes[grpahs_number*7//10:]\n",
    "    for max_depth in range(1, MAX_DECODING_DEPTH+1):\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Training Loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            for i in train_indexes:\n",
    "                graph = dataset_pointer.node_by_id(i)\n",
    "                labels = lg.get_labels(graph, max_depth)\n",
    "                optimizer.zero_grad()\n",
    "                output_whole = model(graph, max_depth)    \n",
    "                output_whole = [j for j,_ in output_whole]\n",
    "                loss = criterion(torch.stack(output_whole).squeeze(1), torch.tensor(labels))/len(labels)\n",
    "                loss.backward()\n",
    "                if (i + 1) % BATCH_SIZE == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(torch.stack(output_whole).squeeze(1), dim=1)\n",
    "                correct += (predictions == torch.tensor(labels)).sum().item()\n",
    "                total += len(labels)\n",
    "            trainig_accuracy = correct / total if total > 0 else 0\n",
    "            \n",
    "            # Testing Loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in test_indexes:\n",
    "                graph = dataset_pointer.node_by_id(i)\n",
    "                labels = lg.get_labels(graph, max_depth)\n",
    "                with torch.no_grad():\n",
    "                    output_whole = model(graph, max_depth=max_depth)\n",
    "\n",
    "                    output_whole = [j for j,_ in output_whole]\n",
    "                    predictions = torch.argmax(torch.stack(output_whole).squeeze(1), dim=1)\n",
    "                    correct += (predictions == torch.tensor(labels)).sum().item()\n",
    "                    total += len(labels)\n",
    "            \n",
    "            accuracy = correct / total if total > 0 else 0\n",
    "            print(f'Max decoding depth: {max_depth}, Epoch {epoch+1}/{EPOCHS}, Training Loss: {total_loss / len(train_indexes)}, TrainingAccuracy: {trainig_accuracy* 100:.2f}%, Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8cd4c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505826307491946"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for i in labels if i[1]==3])/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb63f2f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tuple</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(16, 3)</td>\n",
       "      <td>3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(2, 3)</td>\n",
       "      <td>3540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(16, 2)</td>\n",
       "      <td>2883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(9, 3)</td>\n",
       "      <td>2142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(16, 1)</td>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(9, 2)</td>\n",
       "      <td>1835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(13, 3)</td>\n",
       "      <td>1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(3, 3)</td>\n",
       "      <td>1107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(9, 1)</td>\n",
       "      <td>828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(11, 3)</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(11, 2)</td>\n",
       "      <td>793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(14, 3)</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(3, 2)</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(14, 1)</td>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(13, 2)</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(14, 2)</td>\n",
       "      <td>569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(13, 1)</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(6, 3)</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(8, 3)</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(11, 1)</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10, 2)</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(0, 1)</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(10, 1)</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(18, 2)</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(17, 1)</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(18, 1)</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(6, 2)</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(8, 2)</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(18, 3)</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(10, 3)</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(17, 2)</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(2, 1)</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(17, 3)</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(3, 1)</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(4, 3)</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(15, 1)</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(15, 2)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(12, 1)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(15, 3)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(4, 2)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(12, 2)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(5, 2)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(5, 3)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>(7, 3)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(12, 3)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(7, 2)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>(6, 1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>(7, 1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tuple  Frequency\n",
       "13  (16, 3)       3790\n",
       "6    (2, 3)       3540\n",
       "10  (16, 2)       2883\n",
       "11   (9, 3)       2142\n",
       "0   (16, 1)       2073\n",
       "1    (9, 2)       1835\n",
       "7   (13, 3)       1524\n",
       "5    (2, 2)       1415\n",
       "19   (3, 3)       1107\n",
       "8    (9, 1)        828\n",
       "3   (11, 3)        800\n",
       "17  (11, 2)        793\n",
       "14  (14, 3)        654\n",
       "24   (3, 2)        650\n",
       "9   (14, 1)        607\n",
       "27  (13, 2)        594\n",
       "12  (14, 2)        569\n",
       "29  (13, 1)        473\n",
       "21   (6, 3)        470\n",
       "23   (8, 3)        260\n",
       "32  (11, 1)        211\n",
       "4    (0, 3)        166\n",
       "2   (10, 2)        166\n",
       "22   (0, 1)        164\n",
       "16   (0, 2)        159\n",
       "15  (10, 1)        159\n",
       "39  (18, 2)        128\n",
       "38  (17, 1)        120\n",
       "25  (18, 1)        116\n",
       "30   (6, 2)        111\n",
       "33   (8, 2)         95\n",
       "20  (18, 3)         84\n",
       "26  (10, 3)         83\n",
       "18  (17, 2)         80\n",
       "31   (2, 1)         77\n",
       "35  (17, 3)         62\n",
       "37   (3, 1)         47\n",
       "36   (4, 3)         37\n",
       "42   (1, 3)         22\n",
       "45  (15, 1)         18\n",
       "40  (15, 2)         11\n",
       "28  (12, 1)         10\n",
       "41  (15, 3)          7\n",
       "47   (4, 2)          7\n",
       "46   (1, 2)          6\n",
       "34  (12, 2)          5\n",
       "44   (5, 2)          5\n",
       "48   (5, 3)          4\n",
       "52   (7, 3)          4\n",
       "43  (12, 3)          3\n",
       "49   (7, 2)          2\n",
       "50   (6, 1)          1\n",
       "51   (7, 1)          1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# Count frequencies of each tuple\n",
    "frequency_table = Counter(labels)\n",
    "\n",
    "# Convert the frequency table into a pandas DataFrame\n",
    "df = pd.DataFrame(frequency_table.items(), columns=['Tuple', 'Frequency']).sort_values('Frequency', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "478f003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151227/1298373791.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max decoding depth: 1, Epoch 1/3, Training Loss: 3.0268222567835505, TrainingAccuracy: 69.56%, Test Accuracy: 88.52%\n",
      "Max decoding depth: 1, Epoch 2/3, Training Loss: 2.6563084640404577, TrainingAccuracy: 91.91%, Test Accuracy: 93.54%\n",
      "Max decoding depth: 1, Epoch 3/3, Training Loss: 2.560238545954884, TrainingAccuracy: 94.84%, Test Accuracy: 95.68%\n",
      "Max decoding depth: 2, Epoch 1/3, Training Loss: 1.2131451236901636, TrainingAccuracy: 45.07%, Test Accuracy: 44.89%\n",
      "Max decoding depth: 2, Epoch 2/3, Training Loss: 1.2044079635182245, TrainingAccuracy: 45.89%, Test Accuracy: 46.92%\n",
      "Max decoding depth: 2, Epoch 3/3, Training Loss: 1.1994367652754854, TrainingAccuracy: 52.89%, Test Accuracy: 59.98%\n",
      "Max decoding depth: 3, Epoch 1/3, Training Loss: 0.8402869035606252, TrainingAccuracy: 38.94%, Test Accuracy: 40.82%\n",
      "Max decoding depth: 3, Epoch 2/3, Training Loss: 0.8346686192707277, TrainingAccuracy: 44.21%, Test Accuracy: 47.12%\n",
      "Max decoding depth: 3, Epoch 3/3, Training Loss: 0.8309120531666817, TrainingAccuracy: 48.93%, Test Accuracy: 50.53%\n"
     ]
    }
   ],
   "source": [
    "with data_reader.data_reader(DATASET_PATH) as reader:\n",
    "    dataset_pointer = reader[FILE_PATH]      \n",
    "    grpahs_number = get_file_size(reader, dataset_pointer)\n",
    "    shuffled_indexes = list(range(grpahs_number)) # change indexes to random_shuffle\n",
    "    random.shuffle(shuffled_indexes)\n",
    "    train_indexes = shuffled_indexes[:grpahs_number*7//10]\n",
    "    test_indexes = shuffled_indexes[grpahs_number*7//10:]\n",
    "    for max_depth in range(1, MAX_DECODING_DEPTH+1):\n",
    "        for epoch in range(EPOCHS):\n",
    "            # Training Loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            for i in train_indexes:\n",
    "                graph = dataset_pointer.node_by_id(i)\n",
    "                labels = lg.get_labels(graph, max_depth)\n",
    "                optimizer.zero_grad()\n",
    "                output_whole = model(graph, max_depth=max_depth)\n",
    "                loss = criterion(torch.stack(output_whole).squeeze(1), torch.tensor(labels))/len(labels)\n",
    "                loss.backward()\n",
    "                if (i + 1) % BATCH_SIZE == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                total_loss += loss.item()\n",
    "                predictions = torch.argmax(torch.stack(output_whole).squeeze(1), dim=1)\n",
    "                correct += (predictions == torch.tensor(labels)).sum().item()\n",
    "                total += len(labels)\n",
    "            trainig_accuracy = correct / total if total > 0 else 0\n",
    "            \n",
    "            # Testing Loop\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i in test_indexes:\n",
    "                graph = dataset_pointer.node_by_id(i)\n",
    "                labels = lg.get_labels(graph, max_depth)\n",
    "                with torch.no_grad():\n",
    "                    output_whole = model(graph, max_depth=max_depth)\n",
    "                    predictions = torch.argmax(torch.stack(output_whole).squeeze(1), dim=1)\n",
    "                    correct += (predictions == torch.tensor(labels)).sum().item()\n",
    "                    total += len(labels)\n",
    "            \n",
    "            accuracy = correct / total if total > 0 else 0\n",
    "            print(f'Max decoding depth: {max_depth}, Epoch {epoch+1}/{EPOCHS}, Training Loss: {total_loss / len(train_indexes)}, TrainingAccuracy: {trainig_accuracy* 100:.2f}%, Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de41da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tactician",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
